{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb55f375-5212-4b42-8d5a-b795611d2b05",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87be44e9-8b95-4bd0-b3a7-9a16ff5e29b2",
   "metadata": {},
   "source": [
    "# **Pre-training LLMs with Hugging Face**\n",
    "\n",
    "Estimated time: **45** minutes\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This project aims to introduce you to the process of pretraining large language models (LLMs) using the popular Hugging Face library. Hugging Face is a leading open-source platform for natural language processing that provides a wide range of pretrained models and tools for fine-tuning and deploying these models.\n",
    "\n",
    "You will learn how to load pre-trained models from Hugging Face and make inferences using the Pipeline module. Additionally, you will learn how to further train pre-trained LLMs on your own data (self-supervised fine-tuning). By the end of this lab, you will have a solid understanding of how to pretrain LLMs and store them to later fine-tune for your specific use cases. This will empower you to create powerful and customized natural language processing solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0fd3b2-e7e4-4973-be7e-6dc74d4fa0ab",
   "metadata": {},
   "source": [
    "# __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Pretraining-and-self-supervised-fine-tuning\">Pretraining and self-supervised fine-tuning</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Importing-required-datasets\">Importing required datasets</a></li>\n",
    "            <li><a href=\"#Loading-the-saved-model\">Loading the saved model</a></li>\n",
    "            <li><a href=\"#Inferencing-a-pretrained-BERT-model\">Inferencing a pretrained BERT model</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Exercise\">Exercise</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fae385-2ca5-45c9-bd64-561cc3fd76a4",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6935cf0-3f7f-4675-b71f-92a2abf26404",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "\n",
    " - Load pretrained LLMs from Hugging Face and make inferences using the pipeline module\n",
    " - Train pretrained LLMs on your data \n",
    " - Store LLMs to fine-tune them for specific use cases\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932074fa-dd2d-4975-9001-ccbcdb9ffcd5",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7edb48-5390-4795-a92d-dce431cd31e9",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77176020-85e4-4fe6-a2ac-2f9d867e711b",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!pip` in the code cell below:\n",
    "\n",
    "_PS: To run lab this in your own environment, please note that the versions of libraries may differ due to dependencies._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a649ac-6494-4b93-8237-e89aa1666a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -y\n",
      "Requirement already satisfied: pmdarima in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (2.0.2)\n",
      "Collecting pmdarima\n",
      "  Using cached pmdarima-2.0.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima) (1.4.2)\n",
      "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima) (3.0.11)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.19 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima) (1.13.1)\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima) (0.14.4)\n",
      "Requirement already satisfied: urllib3 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima) (2.3.0)\n",
      "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima) (75.8.0)\n",
      "Requirement already satisfied: packaging>=17.1 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pandas>=0.19->pmdarima) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pandas>=0.19->pmdarima) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pandas>=0.19->pmdarima) (2025.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from scikit-learn>=0.22->pmdarima) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from statsmodels>=0.13.2->pmdarima) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima) (1.17.0)\n",
      "Using cached pmdarima-2.0.4-cp310-cp310-macosx_11_0_arm64.whl (628 kB)\n",
      "Installing collected packages: pmdarima\n",
      "  Attempting uninstall: pmdarima\n",
      "    Found existing installation: pmdarima 2.0.2\n",
      "    Uninstalling pmdarima-2.0.2:\n",
      "      Successfully uninstalled pmdarima-2.0.2\n",
      "Successfully installed pmdarima-2.0.4\n",
      "Collecting pmdarima==2.0.2\n",
      "  Using cached pmdarima-2.0.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima==2.0.2) (1.4.2)\n",
      "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima==2.0.2) (3.0.11)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima==2.0.2) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.19 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima==2.0.2) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima==2.0.2) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima==2.0.2) (1.13.1)\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima==2.0.2) (0.14.4)\n",
      "Requirement already satisfied: urllib3 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima==2.0.2) (2.3.0)\n",
      "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pmdarima==2.0.2) (75.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pandas>=0.19->pmdarima==2.0.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pandas>=0.19->pmdarima==2.0.2) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pandas>=0.19->pmdarima==2.0.2) (2025.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from scikit-learn>=0.22->pmdarima==2.0.2) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from statsmodels>=0.13.2->pmdarima==2.0.2) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from statsmodels>=0.13.2->pmdarima==2.0.2) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima==2.0.2) (1.17.0)\n",
      "Using cached pmdarima-2.0.2-cp310-cp310-macosx_11_0_arm64.whl (579 kB)\n",
      "Installing collected packages: pmdarima\n",
      "  Attempting uninstall: pmdarima\n",
      "    Found existing installation: pmdarima 2.0.4\n",
      "    Uninstalling pmdarima-2.0.4:\n",
      "      Successfully uninstalled pmdarima-2.0.4\n",
      "Successfully installed pmdarima-2.0.2\n"
     ]
    }
   ],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "!pip install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 torch=2.1.0+cu118\n",
    "# - Update a specific package\n",
    "!pip install pmdarima -U\n",
    "# - Update a package to specific version\n",
    "!pip install --upgrade pmdarima==2.0.2\n",
    "# Note: If your environment doesn't support \"!pip install\", use \"!mamba install\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d6f79-d175-4389-a308-a23baf7162d6",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdae7ec9-7191-4517-bd5e-237508037e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /private/var/folders/tr/j1q4ty6x4dn_tfy2pvftfq9c0000gn/T/pip-req-build-rl_n5o06\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /private/var/folders/tr/j1q4ty6x4dn_tfy2pvftfq9c0000gn/T/pip-req-build-rl_n5o06\n",
      "  Resolved https://github.com/huggingface/transformers to commit dd16acb8a3e93b643aa374c9fb80749f5235c1a6\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0.dev0) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests->transformers==4.49.0.dev0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests->transformers==4.49.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests->transformers==4.49.0.dev0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests->transformers==4.49.0.dev0) (2024.12.14)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.49.0.dev0-py3-none-any.whl size=10770282 sha256=f58159e43bcb861212e4ffd6af7b16eae7af4a510cae1a39239113e76713dfb5\n",
      "  Stored in directory: /private/var/folders/tr/j1q4ty6x4dn_tfy2pvftfq9c0000gn/T/pip-ephem-wheel-cache-kfn00ovr/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.48.2\n",
      "    Uninstalling transformers-4.48.2:\n",
      "      Successfully uninstalled transformers-4.48.2\n",
      "Successfully installed transformers-4.49.0.dev0\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.12-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.3.0-py3-none-any.whl (484 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading aiohttp-3.11.12-cp310-cp310-macosx_11_0_arm64.whl (455 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pyarrow-19.0.0-cp310-cp310-macosx_12_0_arm64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\n",
      "Downloading multidict-6.1.0-cp310-cp310-macosx_11_0_arm64.whl (29 kB)\n",
      "Downloading propcache-0.2.1-cp310-cp310-macosx_11_0_arm64.whl (45 kB)\n",
      "Downloading yarl-1.18.3-cp310-cp310-macosx_11_0_arm64.whl (92 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, frozenlist, dill, attrs, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "\u001b[33m  WARNING: The script datasets-cli is installed in '/Users/ankit/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.1.0 datasets-3.3.0 dill-0.3.8 frozenlist-1.5.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.1 pyarrow-19.0.0 xxhash-3.5.0 yarl-1.18.3\n",
      "zsh:1: 2/0.0 not found\n",
      "Collecting torch==2.3.0\n",
      "  Using cached torch-2.3.0-cp310-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch==2.3.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch==2.3.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch==2.3.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch==2.3.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch==2.3.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch==2.3.0) (2024.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from sympy->torch==2.3.0) (1.3.0)\n",
      "Using cached torch-2.3.0-cp310-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "Installing collected packages: torch\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/Users/ankit/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchdata 0.6.0 requires torch==2.0.0, but you have torch 2.3.0 which is incompatible.\n",
      "torchtext 0.15.0 requires torch==2.0.0, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.3.0\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch==2.6.0 (from torchvision)\n",
      "  Using cached torch-2.6.0-cp310-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Downloading torchvision-0.21.0-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.6.0-cp310-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.0\n",
      "    Uninstalling torch-2.3.0:\n",
      "      Successfully uninstalled torch-2.3.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchdata 0.6.0 requires torch==2.0.0, but you have torch 2.6.0 which is incompatible.\n",
      "torchtext 0.15.0 requires torch==2.0.0, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.0.0 torchvision-0.21.0\n",
      "zsh:1: no matches found: protobuf==3.20.*\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers==4.40.0 \n",
    "!pip install -U git+https://github.com/huggingface/transformers\n",
    "!pip install --user datasets # 2.15.0\n",
    "!pip install --user portalocker>=2/0.0\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install --user torch==2.3.0\n",
    "!pip install -U torchvision\n",
    "!pip install --user protobuf==3.20.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad9f5508-e0bf-4987-bbe8-5852053c9ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/ankit/.local/lib/python3.10/site-packages (3.3.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/ankit/.local/lib/python3.10/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/ankit/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/ankit/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/ankit/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /Users/ankit/.local/lib/python3.10/site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/ankit/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ankit/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/ankit/.local/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ankit/.local/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ankit/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ankit/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/ankit/.local/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/ankit/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17afb8b8-9845-45f9-9a55-f6c6914c1a27",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "_It is recommended that you import all required libraries in one place (here):_\n",
    "* Note: if you got an error after running the cell below, try restarting the Kernel as some packages need a restart to be effective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64222c23-8659-4065-8bb6-f0ab3cffbe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoConfig,AutoModelForCausalLM,AutoModelForSequenceClassification,BertConfig,BertForMaskedLM,TrainingArguments, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer,BertTokenizerFast,TextDataset,DataCollatorForLanguageModeling\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df83eff-a8dd-48bf-a9f4-c73d46e55f70",
   "metadata": {},
   "source": [
    "Disable tokenizer parallelism to avoid deadlocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ceff39-8d80-4270-903c-382f626c3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable TOKENIZERS_PARALLELISM to 'false'\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42395d00-169f-4b54-91fc-421ad6bd8599",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab4707-c193-41e0-9ef4-5679ca9f65e4",
   "metadata": {},
   "source": [
    "# Pretraining and self-supervised fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac96e20-79fc-41fd-bbf7-a39f4a723de5",
   "metadata": {},
   "source": [
    "Pretraining is a technique used in natural language processing (NLP) to train large language models (LLMs) on a vast corpus of unlabeled text data. The goal is to capture the general patterns and semantic relationships present in natural language, allowing the model to develop a deep understanding of language structure and meaning.\n",
    "\n",
    "The motivation behind pretraining transformers is to address the limitations of traditional NLP approaches that often require significant amounts of labeled data for each specific task. By leveraging the abundance of unlabeled text data, pretraining enables the model to learn fundamental language skills through self-supervised objectives, facilitating transfer learning.\n",
    "\n",
    "The pretraining objectives, such as masked language modeling (MLM) and next sentence prediction (NSP), play a crucial role in the success of transformer models. Pretrained models can be further tuned by training them on domain-specific unlabeled data, which is known as self-supervised fine-tuning.\n",
    "\n",
    "Also, the model can be fine-tuned on specific downstream tasks using labeled data, a process known as supervised fine-tuning, further improving its performance.\n",
    "\n",
    "In the following sections of this lab, you will explore pretraining objectives, loading pretrained models, data preparation, and the fine-tuning process. By the end, you will have a solid understanding of pretraining and self-supervised fine-tuning, empowering you to apply these techniques to solve real-world NLP problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeaa7bd-f38e-4056-9f6c-3b9e4669a0b1",
   "metadata": {},
   "source": [
    "Let's start with loading a pretrained model from Hugging Face and making an inference:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6be3b56-c8a0-4837-b7a5-f3c42b9ca74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was really good. I was really surprised by how good it was.\n",
      "I was\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model,tokenizer=tokenizer)\n",
    "print(pipe(\"This movie was really\")[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10330a7d-20c8-4ee5-aee1-6f24019889bf",
   "metadata": {},
   "source": [
    "## Pre-training Objectives\n",
    "\n",
    "Pre-training objectives are crucial components of the pre-training process for transformers. These objectives define the tasks that the model is trained on during the pre-training phase, allowing it to learn meaningful contextual representations of language. Three commonly used pre-training objectives are masked language modeling (MLM), next sentence prediction (NSP) and next Ttoken prediction.\n",
    "\n",
    "1. Masked Language Modeling (MLM):\n",
    "   Masked language modeling involves randomly masking some words in a sentence and training the model to predict the masked words based on the context provided by the surrounding words(i.e., words that appear either before or after the masked word). The objective is to enable the model to learn contextual understanding and fill in missing information.\n",
    "\n",
    "2. Next Sentence Prediction (NSP):\n",
    "   Next sentence prediction involves training the model to predict whether two sentences are consecutive in the original text or randomly chosen from the corpus. This objective helps the model learn sentence-level relationships and understand the coherence between sentences.\n",
    "\n",
    "3. Next Token Prediction:\n",
    "    In this objective, the model is trained to predict the next token in a sequence of text. The model is presented with a sequence of text and must learn to predict the most likely next token based on the context.\n",
    "\n",
    "It's important to note that different pre-trained models may use variations or combinations of these objectives, depending on the specific architecture and training setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760f706-2a71-49c0-9c5e-dc560692b6d7",
   "metadata": {},
   "source": [
    "## Self-supervised training of a BERT model\n",
    "Training a BERT(Bidirectional Encoder Representations from Transformers) model is a complex and time-consuming process that requires a large corpus of unlabeled text data and significant computational resources. However, we provide you with a simplified exercise to demonstrate the steps involved in pre-training a BERT model using the Masked Language Modeling (MLM) objective.\n",
    "\n",
    "For this exercise, we'll use the Hugging Face Transformers library, which provides pre-implemented BERT models and tools for pre-training. You will be instructed to:\n",
    "- Prepare the train dataset\n",
    "- Train a Tokenizer\n",
    "- Preprocess the dataset\n",
    "- Pre-train BERT using an MLM task\n",
    "- Evaluate the trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f9036a-2387-4f59-88e9-a2f7dc75ca9b",
   "metadata": {},
   "source": [
    "### Importing required datasets\n",
    "\n",
    "The WikiText dataset is a widely used benchmark dataset in the field of natural language processing (NLP). The dataset contains a large amount of text extracted from Wikipedia, which is a vast online encyclopedia covering a wide range of topics. The articles in the WikiText dataset are preprocessed to remove formatting, hyperlinks, and other metadata, resulting in a clean text corpus.\n",
    "\n",
    "The WikiText dataset has 4 different configs, and is divided into three parts: a training set, a validation set, and a test set. The training set is used for training language models, while the validation and test sets are used for evaluating the performance of the models.\n",
    "First, let's load the datasets and concatenate them together to create a big dataset.\n",
    "\n",
    "*Note: The original BERT was pretrained on Wikipedia and BookCorpus datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55ee3455-e756-4ee3-b685-00925c476706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff3295b-a887-48a9-851d-eafa17eaade2",
   "metadata": {},
   "source": [
    "Let's check the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8114272c-0b43-456a-a87a-2054bf5fcf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92ff53-0c97-4060-8ec2-e64381c98854",
   "metadata": {},
   "source": [
    "check a sample record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da2acac5-5f95-49a2-802c-8fe325b7674d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" When Mason was injured in warm @-@ ups late in the year , Columbus was without an active goaltender on their roster . To remedy the situation , the team signed former University of Michigan goaltender Shawn Hunwick to a one @-@ day , amateur tryout contract . After being eliminated from the NCAA Tournament just days prior , Hunwick skipped an astronomy class and drove his worn down 2003 Ford Ranger to Columbus to make the game . He served as the back @-@ up to Allen York during the game , and the following day , he signed a contract for the remainder of the year . With Mason returning from injury , Hunwick was third on the team 's depth chart when an injury to York allowed Hunwick to remain as the back @-@ up for the final two games of the year . In the final game of the season , the Blue Jackets were leading the Islanders 7 – 3 with 2 : 33 remaining when , at the behest of his teammates , Head Coach Todd Richards put Hunwick in to finish the game . He did not face a shot . Hunwick was the franchise record ninth player to make his NHL debut during the season . Conversely , Vaclav Prospal played in his 1,000th NHL game during the year . \\n\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check a sample record\n",
    "dataset[\"train\"][400]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c2e3db-7920-4018-ae51-800daa1a2e9c",
   "metadata": {},
   "source": [
    "This dataset contains 36,718 rows of training data. If you do not run the code on a GPU-powered notebook, you will need to decrease the size of dataset to be able to complete the training. You can uncomment the commands below to select a desired section of dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbdfc731-762e-40e4-9486-1f1fb7282504",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].select([i for i in range(400)])\n",
    "dataset[\"test\"] = dataset[\"test\"].select([i for i in range(80)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de33cef-f95b-4612-bfd4-a9d1c8ee2485",
   "metadata": {},
   "source": [
    "Below files are next used in creating TextDataset objects for the training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80f3b3d1-8ab1-4c10-b51f-79dfc90612d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the datasets to text files\n",
    "output_file_train = \"wikitext_dataset_train.txt\"\n",
    "output_file_test = \"wikitext_dataset_test.txt\"\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file_train, \"w\", encoding=\"utf-8\") as f:\n",
    "    # Iterate over each example in the dataset\n",
    "    for example in dataset[\"train\"]:\n",
    "        # Write the example text to the file\n",
    "        f.write(example[\"text\"] + \"\\n\")\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file_test, \"w\", encoding=\"utf-8\") as f:\n",
    "    # Iterate over each example in the dataset\n",
    "    for example in dataset[\"test\"]:\n",
    "        # Write the example text to the file\n",
    "        f.write(example[\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02c8c9-e09e-4c71-b244-382f7134994f",
   "metadata": {},
   "source": [
    "You need to define a tokenizer to be used for tokenizing the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68b9b689-227e-41b2-8b56-5e76fb319921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer from existing one to re-use special tokens\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6c50b66-7a9d-451f-8e9a-089c5b3486f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, is_decoder=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776d2f0-a4fa-49d7-ac4e-48423396e1b9",
   "metadata": {},
   "source": [
    "### Training a Tokenizer(Optional)\n",
    "\n",
    "In the previous cell, you created an instance of tokenizer from a pre-trained BERT tokenizer. If you want to train the tokenizer on your own dataset, you can uncomment the code below. This is specially helpful when using transformers for specific areas such as medicine where tokens are somehow different than the general tokens that tokenizers are created based on. (You can skip this step if you do not want to train the tokenizer on your specific data):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36f1811e-b083-4282-94ae-a4bfb03e2f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 32.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## create a python generator to dynamically load the data\n",
    "def batch_iterator(batch_size=10000):\n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        yield dataset['train'][i : i + batch_size][\"text\"]\n",
    "\n",
    "## create a tokenizer from existing one to re-use special tokens\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "## train the tokenizer using our own dataset\n",
    "bert_tokenizer = bert_tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=30522)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7fca84-4716-4dcd-b405-c8aa36846d80",
   "metadata": {},
   "source": [
    "### Pretraining\n",
    "\n",
    "In this step, we define the configuration of the BERT model and create the model:\n",
    "#### Define the BERT Configuration\n",
    "Here, we define the configuration settings for a BERT model using `BertConfig`. This includes setting various parameters related to the model's architecture:\n",
    "- **vocab_size=30522**: Specifies the size of the vocabulary. This number should match the vocabulary size used by the tokenizer.\n",
    "- **hidden_size=768**: Sets the size of the hidden layers.\n",
    "- **num_hidden_layers=12**: Determines the number of hidden layers in the transformer model.\n",
    "- **num_attention_heads=12**: Sets the number of attention heads in each attention layer.\n",
    "- **intermediate_size=3072**: Specifies the size of the \"intermediate\" (i.e., feed-forward) layer within the transformer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7d24534-6ef7-450b-b8fb-ae7fee99d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BERT configuration\n",
    "config = BertConfig(\n",
    "    vocab_size=len(bert_tokenizer.get_vocab()),  # Specify the vocabulary size(Make sure this number equals the vocab_size of the tokenizer)\n",
    "    hidden_size=768,  # Set the hidden size\n",
    "    num_hidden_layers=12,  # Set the number of layers\n",
    "    num_attention_heads=12,  # Set the number of attention heads\n",
    "    intermediate_size=3072,  # Set the intermediate size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e8b2f8-1875-48cd-8138-94552fbf0e7a",
   "metadata": {},
   "source": [
    " Create the BERT model for pre-training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc0068c9-1b3a-416d-a952-5f1744bb8f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BERT model for pre-training\n",
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd094a-e4e9-46ed-a38c-754bfa6443b0",
   "metadata": {},
   "source": [
    "check model configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcc4d167-6ed8-4238-8bf9-d1a7aaee318c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model configuration\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79c588-4572-4fb7-8d77-2e8d59e6ea92",
   "metadata": {},
   "source": [
    "### Define the Training Dataset\n",
    "Here, we define a training dataset using the `TextDataset` class, which is suited for loading and processing text data for training language models. This setup typically involves a few key parameters:\n",
    "\n",
    "- **tokenizer=bert_tokenizer**: Specifies the tokenizer to be used. Here, `bert_tokenizer` is an instance of a BERT tokenizer, responsible for converting text into tokens that the model can understand.\n",
    "- **file_path=\"wikitext_dataset_train.txt\"**: The path to the pre-training data file. This should point to a text file containing the training data.\n",
    "- **block_size=128**: Sets the desired block size for training. This defines the length of the sequences that the model will be trained on\n",
    "\n",
    "The `TextDataset` class is designed to take large pieces of text (such as those found in the specified file), tokenize them, and efficiently handle them in manageable blocks of the specified size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acb32346-1323-497b-bf5f-396adc9065ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the pre-training data as a TextDataset\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    file_path=\"wikitext_dataset_train.txt\",  # Path to your pre-training data file\n",
    "    block_size=128  # Set the desired block size for training\n",
    ")\n",
    "test_dataset = TextDataset(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    file_path=\"wikitext_dataset_test.txt\",  # Path to your pre-training data file\n",
    "    block_size=128  # Set the desired block size for training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313e4a7f-2aac-41c6-a47f-b75cbfa2b394",
   "metadata": {},
   "source": [
    "examining  one sample the token indexes  are shown here with the block size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05d9a343-3440-4c02-837d-992821232942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   29,  311,  374,  698,   29, 1581,  394,  311,   20,   27, 6009,\n",
       "         374,   10, 1390,   27,   97,   95, 2453,   13, 1158,   15,  311,  178,\n",
       "         163, 1124,   20,   11,   13, 5261, 1788,  182,  216,  311,  374,  698,\n",
       "        1625,  954,   13,  263,   33, 2472, 1375,   30,   14,   30, 1338, 1970,\n",
       "         253, 1845,  242, 1530,  180, 1933,   15, 1470,  197,  163, 1000, 2367,\n",
       "          15,  878,  176, 1139,  638,  176,  954,   13,  278,  263,  163,  782,\n",
       "         253,  176,  163,  311,  639,   15, 5358,  163,  846, 5798,  178, 2472,\n",
       "         180, 1325,   30,   14,   30,  496,  867,  216,  344, 3319,   13,  163,\n",
       "         694, 3757, 4958,  182,  163,  349,  253,  180, 4818,  163,    5,  830,\n",
       "           5,   13,   33, 1589,  704,  524, 2938,  163,  529,  178, 1587,  385,\n",
       "         163,  617, 3381,  481,  540, 1546, 3001,    3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5217de2a-1b3d-463d-a3d3-dc0a8e60c430",
   "metadata": {},
   "source": [
    "Then, we prepare data for the MLM task (masking random tokens):\n",
    "### Define the Data Collator for Language Modeling\n",
    "This line of code sets up a `DataCollatorForLanguageModeling` from the Hugging Face Transformers library. A data collator is used during training to dynamically create batches of data. For language modeling, particularly for models like BERT that use masked language modeling (MLM), this collator prepares training batches by automatically masking tokens according to a specified probability. Here are the details of the parameters used:\n",
    "\n",
    "- **tokenizer=bert_tokenizer**: Specifies the tokenizer to be used with the data collator. The `bert_tokenizer` is responsible for tokenizing the text and converting it to the format expected by the model.\n",
    "- **mlm=True**: Indicates that the data collator should mask tokens for masked language modeling training. This parameter being set to `True` configures the collator to randomly mask some of the tokens in the input data, which the model will then attempt to predict.\n",
    "- **mlm_probability=0.15**: Sets the probability with which tokens will be masked. A probability of 0.15 means that, on average, 15% of the tokens in any sequence will be replaced with a mask token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae1dc6a5-c876-479e-ad4e-5f23fff7d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=bert_tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af29b1f6-15f3-4570-bc84-655be520203c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2,   29,  103,  374,  698,  103, 1581,  394,  311,   20,   27, 6009,\n",
       "           374,   10, 1390,   27,   97,   95, 2453,  103, 1158,   15,  311,  178,\n",
       "           103, 1124,   20,   11,   13, 5261, 1788,  182,  216,  311,  374,  698,\n",
       "          1625,  954,  103,  263,   33, 2472, 1375,   30,   14,   30, 1338,  103,\n",
       "           253, 1845,  242, 1530,  180, 1933,   15, 1470,  197,  163, 1000, 2367,\n",
       "            15,  103,  176, 1139,  638,  176,  954,   13,  278,  263,  163,  782,\n",
       "           253,  103,  163,  311,  639,  103, 5358,  163,  846,  103,  103, 2472,\n",
       "           180, 1325,   30,   14,   30,  496,  867,  103,  344, 3319,   13,  163,\n",
       "           694,  103,  103,  182,  163,  103,  253,  180, 4818,  163,    5,  830,\n",
       "             5,   13,   33, 1589,  704,  524, 2938,  103,  529,  178, 1587,  385,\n",
       "           163,  617, 3381,  103,  540, 1546, 3001,  103]]),\n",
       " 'labels': tensor([[-100, -100,  311, -100, -100,   29, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100,   97, -100, -100,   13, -100, -100, -100, -100,\n",
       "           163, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100,   13, -100, -100, -100, -100, -100, -100, -100, -100, 1970,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100,  878, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100,  176, -100, -100, -100,   15, -100, -100, -100, 5798,  178, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100,  216, -100, -100, -100, -100,\n",
       "          -100, 3757, 4958, -100, -100,  349, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100,  163, -100, -100, -100, -100,\n",
       "          -100, -100, -100,  481, -100, -100, -100,    3]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how collator transforms a sample input data record\n",
    "data_collator([train_dataset[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799dad82-0ad8-4547-8d5f-a2190832b0ac",
   "metadata": {},
   "source": [
    "Now, we train the BERT Model using the Trainer module. (For a complete list of training arguments, check [here](https://huggingface.co/docs/transformers/v4.33.2/en/main_classes/trainer#transformers.TrainingArguments)):\n",
    "This section configures the training process by specifying various parameters that control how the model is trained, evaluated, and saved:\n",
    "\n",
    "- **output_dir=\"./trained_model\"**: Specifies the directory where the trained model and other output files will be saved.\n",
    "- **overwrite_output_dir=True**: If set to `True`, this will overwrite the contents of the output directory if it already exists. This is useful when running experiments multiple times.\n",
    "- **do_eval=True**: Enables evaluation of the model. If `True`, the model will be evaluated at the specified intervals.\n",
    "- **evaluation_strategy=\"epoch\"**: Defines when the model should be evaluated. Setting this to \"epoch\" means the model will be evaluated at the end of each epoch.\n",
    "- **learning_rate=5e-5**: Sets the learning rate for training the model. This is a typical learning rate for fine-tuning BERT-like models.\n",
    "- **num_train_epochs=10**: Specifies the number of training epochs. Each epoch involves a full pass over the training data.\n",
    "- **per_device_train_batch_size=2**: Sets the batch size for training on each device. This should be set based on the memory capacity of your hardware.\n",
    "- **save_total_limit=2**: Limits the total number of model checkpoints to be saved. Only the most recent two checkpoints will be kept.\n",
    "- **logging_steps=20**: Determines how often to log training information, which can help monitor the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4fb82-55e6-47eb-b12f-4afd51bded9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained_model\",  # Specify the output directory for the trained model\n",
    "    overwrite_output_dir=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,  # Specify the number of training epochs\n",
    "    per_device_train_batch_size=2,  # Set the batch size for training\n",
    "    save_total_limit=2,  # Limit the total number of saved checkpoints\n",
    "    logging_steps = 20\n",
    "    \n",
    ")\n",
    "\n",
    "# Instantiate the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Start the pre-training\n",
    "trainer.train()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67a656-1342-408c-9573-e4eb8ce71f1d",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance\n",
    "\n",
    "Let's check the performance of the trained model. Perplexity is commonly used to compare different language models or different configurations of the same model.\n",
    "After training, perplexity can be calculated on a held-out evaluation dataset to assess the model's performance. The perplexity is calculated by feeding the evaluation dataset through the model and comparing the predicted probabilities of the target tokens with the actual token values that are masked.\n",
    "\n",
    "A lower perplexity score indicates that the model has a better understanding of the language and is more effective at predicting the masked tokens. It suggests that the model has learned useful representations and can generalize well to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba26e28d-17d1-4281-be09-b332616666a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075194dd-cb14-4dce-8275-f93b40e7a8a0",
   "metadata": {},
   "source": [
    "## Loading the saved model\n",
    "If you want to skip training and load the model that you trained for 10 epochs, go ahead and uncomment the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "feb2625a-f6d8-41c1-befd-240feb471e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-15 15:32:39--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/BeXRxFT2EyQAmBHvxVaMYQ/bert-scratch-model.pt\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 438141816 (418M) [binary/octet-stream]\n",
      "Saving to: ‘bert-scratch-model.pt.1’\n",
      "\n",
      "bert-scratch-model. 100%[===================>] 417.84M  48.8MB/s    in 8.7s    \n",
      "\n",
      "2025-02-15 15:32:48 (48.1 MB/s) - ‘bert-scratch-model.pt.1’ saved [438141816/438141816]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/BeXRxFT2EyQAmBHvxVaMYQ/bert-scratch-model.pt'\n",
    "model.load_state_dict(torch.load('bert-scratch-model.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3df9e0-09f9-49ed-a168-aab5186b0052",
   "metadata": {},
   "source": [
    "The simplest way to try out the model for inference is to use it in a pipeline(). Instantiate a pipeline for fill-mask with your model, and pass your text to it. If you like, you can use the top_k parameter to specify how many predictions to return:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fe0ba42-f92d-468a-9872-6c97d94cdceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: [unused539], Confidence: 0.04\n",
      "Predicted token: [unused17], Confidence: 0.04\n",
      "Predicted token: [unused15], Confidence: 0.03\n",
      "Predicted token: [unused557], Confidence: 0.02\n",
      "Predicted token: [unused551], Confidence: 0.02\n"
     ]
    }
   ],
   "source": [
    "# Define the input text with a masked token\n",
    "text = \"This is a [MASK] movie!\"\n",
    "\n",
    "# Create a pipeline for the \"fill-mask\" task\n",
    "mask_filler = pipeline(\"fill-mask\", model=model,tokenizer=bert_tokenizer)\n",
    "\n",
    "# Generate predictions by filling the mask in the input text\n",
    "results = mask_filler(text) #top_k parameter can be set \n",
    "\n",
    "# Print the predicted sequences\n",
    "for result in results:\n",
    "    print(f\"Predicted token: {result['token_str']}, Confidence: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bed696-44c3-45d9-b2df-748c27710740",
   "metadata": {},
   "source": [
    "You can see that [MASK] is replaced by the most frequent token. This weak performance can be due to insufficient training, lack of training data, model architecture, or not tuning hyperparameters. Let's try a pretrained model from Hugging Face:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db3ac26-4c13-489b-a59e-886d729f6fae",
   "metadata": {},
   "source": [
    "## Inferencing a pretrained BERT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83b71cda-a410-45e5-bb54-103e9b35e63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: great, Confidence: 0.16\n",
      "Predicted token: horror, Confidence: 0.08\n",
      "Predicted token: good, Confidence: 0.08\n",
      "Predicted token: bad, Confidence: 0.05\n",
      "Predicted token: fantastic, Confidence: 0.04\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained BERT model and tokenizer\n",
    "pretrained_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "pretrained_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the input text with a masked token\n",
    "text = \"This is a [MASK] movie!\"\n",
    "\n",
    "# Create the pipeline\n",
    "mask_filler = pipeline(task='fill-mask', model=pretrained_model,tokenizer=pretrained_tokenizer)\n",
    "\n",
    "# Perform inference using the pipeline\n",
    "results = mask_filler(text)\n",
    "for result in results:\n",
    "    print(f\"Predicted token: {result['token_str']}, Confidence: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae5d74-b786-4ad5-89b9-4d13c0e880a8",
   "metadata": {},
   "source": [
    "This pretrianed model performs way better than the model you just trained for a few epochs using a single dataset. Still, pretrained models cannot be used for specific tasks, such as sentiment extraction or sequence classification. This is why supervised fine-tuning methods are introduced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215c81c-f774-4969-8c91-faf01f9faa59",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc5d889-8991-4e02-bf7d-db798e326449",
   "metadata": {},
   "source": [
    "## Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac31c94-92b5-4e40-bed8-cd91b7530d5a",
   "metadata": {},
   "source": [
    "1. Create a model and tokenizer using Hugging Face library.\n",
    "2. Go to this [link](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=trending)\n",
    "3. Choose a Text Classification dataset that you can load, for instance 'stanfordnlp/snli'\n",
    "4. Use that dataset to train your model(please be mindful of the resources available for the training) and evaluate it.\n",
    "\n",
    "   >Note: The lab environment doesn't have the resources to support the training and this might cause the kernel to die.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6f15731-c558-4868-8513-da29a9b1241f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 10000/10000 [00:00<00:00, 149374.06 examples/s]\n",
      "Generating validation split: 100%|██████████| 10000/10000 [00:00<00:00, 1314746.41 examples/s]\n",
      "Generating train split: 100%|██████████| 550152/550152 [00:00<00:00, 4626900.60 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 550152/550152 [01:32<00:00, 5927.35 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 5953.93 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='275076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     8/275076 06:18 < 4818:22:09, 0.02 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 40\u001b[0m\n\u001b[1;32m     26\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     27\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Replace with your output directory\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     29\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     30\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     35\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     36\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_encoded,\n\u001b[1;32m     37\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_encoded,\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Evaluation function (replace with your metrics)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/transformers/trainer.py:2599\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2595\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped:\n\u001b[1;32m   2604\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/accelerate/optimizer.py:178\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:140\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    139\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/torch/optim/adamw.py:243\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    230\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    232\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    233\u001b[0m         group,\n\u001b[1;32m    234\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m         state_steps,\n\u001b[1;32m    241\u001b[0m     )\n\u001b[0;32m--> 243\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/torch/optim/adamw.py:875\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 875\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/torch/optim/adamw.py:479\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    477\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 479\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SNLI dataset\n",
    "snli = load_dataset(\"stanfordnlp/snli\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "  premise = examples[\"premise\"]\n",
    "  hypothesis = examples[\"hypothesis\"]\n",
    "  return tokenizer(premise, hypothesis, padding=\"max_length\", truncation=True)\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Apply preprocessing to training and validation sets\n",
    "train_encoded = snli[\"train\"].map(preprocess_function, batched=True)\n",
    "val_encoded = snli[\"validation\"].map(preprocess_function, batched=True)\n",
    "\n",
    "# Training function (replace with your training loop)\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Replace with your output directory\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_encoded,\n",
    "    eval_dataset=val_encoded,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluation function (replace with your metrics)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions, labels = trainer.predict(val_encoded)\n",
    "accuracy = accuracy_score(labels, predictions.argmax(-1))\n",
    "print(f\"Accuracy on validation set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152ef5b-597f-41d2-a12d-248822dbf15b",
   "metadata": {},
   "source": [
    "# Congratulations! You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3eeb18-1153-44c3-a43e-f6add7540e81",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83af8af-1691-44e1-997c-a3a2dcf9c07a",
   "metadata": {},
   "source": [
    "[Fateme Akbari](https://author.skills.network/instructors/fateme_akbari)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833cfbd9-ef1a-4c93-8781-154733096fa1",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "prev_pub_hash": "12be72596f22967736837e28a7be96d86f7db37dcd84a70c4ed644d41af75003"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
